# Principles of AI: LLMs (UPenn, Stat 9911, Spring 2025)

Instructor: [Edgar Dobriban](https://statistics.wharton.upenn.edu/profile/dobriban/)

This course explore Large Language Models (LLMs), from the basics to cutting-edge research. 

## Reference Materials
- [Course Syllabus](https://github.com/dobriban/Principles-of-AI-LLMs/blob/main/Lec/syllabus.pdf). 
- [Lecture  Notes](https://github.com/dobriban/Principles-of-AI-LLMs/blob/main/Lec/Stat_9911_Principles_of_AI.pdf); Work in progress.

## Lectures

Here is the reformatted table with the lecture links in the first column and the "Lecture" column removed:

| Link | Topic |
|------|------------------------------------------------------------|
| [ðŸ“„ Lec 01](https://github.com/dobriban/Principles-of-AI-LLMs/blob/main/Lec/Stat_9911_Lec_01.pdf) | Motivation and Context |
| [ðŸ“„ Lec 02](https://github.com/dobriban/Principles-of-AI-LLMs/blob/main/Lec/Stat_9911_Lec_02.pdf) | AI: Goals and definitions. The role of LLMs. |
| [ðŸ“„ Lec 03](https://github.com/dobriban/Principles-of-AI-LLMs/blob/main/Lec/Stat_9911_Lec_03.pdf) | LLM architectures: attention and transformers. |
| [ðŸ“„ Lec 04](https://github.com/dobriban/Principles-of-AI-LLMs/blob/main/Lec/Stat_9911_Lec_04.pdf) | Insight into transformer architectures. |
| [ðŸ“„ Lec 05](https://github.com/dobriban/Principles-of-AI-LLMs/blob/main/Lec/Stat_9911_Lec_05.pdf) | Position encoding. |
| [ðŸ“„ Lec 06](https://github.com/dobriban/Principles-of-AI-LLMs/blob/main/Lec/Stat_9911_Lec_06.pdf) | Specific LLM families: GPT, Llama, DeepSeek, LLM360. |
| [ðŸ“„ Lec 07](https://github.com/dobriban/Principles-of-AI-LLMs/blob/main/Lec/Stat_9911_Lec_07.pdf) | Training LLMs: pre- and post-training, supervised fine-tuning, learning from preferences (PPO, DPO, GRPO). |
| [ðŸ“„ Lec 08](https://github.com/dobriban/Principles-of-AI-LLMs/blob/main/Lec/Stat_9911_Lec_08.pdf) | Test-time computation: sampling, prompting, reasoning. |
| [ðŸ“„ Lec 09](https://github.com/dobriban/Principles-of-AI-LLMs/blob/main/Lec/Stat_9911_Lec_09.pdf) | Empirical Behaviors: scaling laws, emergence, memorization, super-phenomena. |

## Student Presentations
After initial lectures, students will lead presentations on topics of their choice in recent advances or research questions in AI. 

## Additional Resources
### Links to other courses
- [Foundations of Large Language Models](https://www.dropbox.com/scl/fo/v3jbijgpew64vv77cpwen/h?rlkey=hx1ux02uvhzdpq6tmbvo0bsuk&e=1&dl=0), U of Michigan, 2024
- [Language Modeling from Scratch](https://stanford-cs336.github.io/spring2024/), Stanford, Spring 2024
- [Recent Advances on Foundation Models](https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/), U of Waterloo, Winter 2024
- [Large Models](https://www.cs.toronto.edu/~cmaddis/courses/csc2541_w25/), U of Toronto, Winter 2025


### Videos 
- Andrej Karpathy's [Neural Networks: Zero to Hero](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ) video lectures. 100% coding-based, hands-on tutorial on implementing basic autodiff, neural nets, language models, and GPT-2 mini (124M params). 

### Key papers
- [The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783) describes the Llama "open-weights LLM" developed by Meta. Possibly the highest information content anywhere about LLMs.
- [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437v1) and [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning
](https://arxiv.org/abs/2501.12948) describe the open-weights DeepSeek V3 and R1 models, which bring together several innovations in training LLMs to make them achieve comparable performance to some top closed models.

### Tutorials, books and book chapters 
- The corresponding sections in the [Understanding Deep Learning](https://udlbook.github.io/udlbook/) book. See also the associated tutorial posts: [LLMs](https://www.borealisai.com/research-blogs/a-high-level-overview-of-large-language-models/); Transformers [1](https://www.borealisai.com/research-blogs/tutorial-14-transformers-i-introduction/), [2](https://www.borealisai.com/research-blogs/tutorial-16-transformers-ii-extensions/), [3](https://www.borealisai.com/research-blogs/tutorial-17-transformers-iii-training/); [Training and fine-tuning](https://www.borealisai.com/research-blogs/training-and-fine-tuning-large-language-models/);  [Inference](https://www.borealisai.com/research-blogs/speeding-up-inference-in-transformers/)
- [Foundations of Large Language Models](https://arxiv.org/abs/2501.09223) book

### Workshops, conferences
[NeurIPS](https://nips.cc/), [ICML](https://icml.cc/), [ICLR](https://iclr.cc/)


